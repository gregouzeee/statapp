"""
Adaptation simplifiÃ©e de SelfCheckGPT-Prompt pour GPT2
BasÃ© sur : https://github.com/potsawee/selfcheckgpt
"""

import torch
import numpy as np
from tqdm import tqdm
from typing import List
from transformers import AutoTokenizer, AutoModelForCausalLM


class SelfCheckPrompt:
    """
    Version simplifiÃ©e de SelfCheckGPT-Prompt
    Compatible avec GPT2 et autres modÃ¨les causaux
    """
    
    def __init__(
        self,
        model_name: str = "gpt2",
        device: str = None,
        prompt_template: str = None
    ):
        """
        Initialise le modÃ¨le pour SelfCheck
        
        Args:
            model_name: Nom du modÃ¨le HuggingFace (ex: "gpt2", "gpt2-medium")
            device: "cpu" ou "cuda". Si None, dÃ©tection automatique
            prompt_template: Template custom (optionnel)
        """
        print(f"ðŸ”„ Chargement du modÃ¨le {model_name}...")
        
        # DÃ©tection automatique du device
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)
        
        # Chargement du tokenizer
        print("ðŸ“ Chargement du tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # NÃ©cessaire pour GPT2 (pas de pad_token par dÃ©faut)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Chargement du modÃ¨le
        print("ðŸ§  Chargement du modÃ¨le...")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto"
        )
        self.model.eval()  # Mode Ã©valuation
        self.model.to(self.device)
        
        # Template de prompt
        if prompt_template is None:
            self.prompt_template = (
                "Context: {context}\n\n"
                "Sentence: {sentence}\n\n"
                "Is the sentence supported by the context above? "
                "Answer Yes or No.\n\n"
                "Answer:"
            )
        else:
            self.prompt_template = prompt_template
        
        # Mapping rÃ©ponse -> score
        self.text_mapping = {
            'yes': 0.0,  # CohÃ©rent
            'no': 1.0,   # IncohÃ©rent (hallucination)
            'n/a': 0.5   # Incertain
        }
        
        # Pour tracker les rÃ©ponses inattendues
        self.unknown_responses = set()
        
        print(f"âœ… ModÃ¨le {model_name} chargÃ© sur {self.device}")
        print(f"ðŸ’¾ MÃ©moire GPU utilisÃ©e: {self._get_gpu_memory()}")
    
    def _get_gpu_memory(self) -> str:
        """Retourne la mÃ©moire GPU utilisÃ©e (si disponible)"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3  # GB
            return f"{allocated:.2f} GB"
        return "N/A (CPU)"
    
    @torch.no_grad()
    def predict(
        self,
        sentences: List[str],
        sampled_passages: List[str],
        verbose: bool = True,
        max_new_tokens: int = 5,
        batch_responses: bool = False
    ) -> np.ndarray:
        """
        Calcule les scores d'inconsistance pour chaque phrase
        
        Args:
            sentences: Liste des phrases Ã  Ã©valuer
            sampled_passages: Liste des passages Ã©chantillonnÃ©s
            verbose: Afficher la progression
            max_new_tokens: Nombre max de tokens Ã  gÃ©nÃ©rer
            batch_responses: Si True, retourne aussi les rÃ©ponses brutes
            
        Returns:
            scores: Array numpy de shape (len(sentences),)
                   Valeurs entre 0.0 (cohÃ©rent) et 1.0 (incohÃ©rent)
        """
        num_sentences = len(sentences)
        num_samples = len(sampled_passages)
        
        # Matrice pour stocker tous les scores
        scores_matrix = np.zeros((num_sentences, num_samples))
        
        # Optionnel : stocker les rÃ©ponses brutes
        if batch_responses:
            all_responses = []
        
        # Boucle sur les phrases
        iterator = tqdm(range(num_sentences), disable=not verbose, 
                       desc="Ã‰valuation des phrases")
        
        for sent_idx in iterator:
            sentence = sentences[sent_idx]
            
            if batch_responses:
                sentence_responses = []
            
            # Boucle sur les Ã©chantillons
            for sample_idx, sample in enumerate(sampled_passages):
                
                # 1. Construction du prompt
                sample_clean = sample.replace("\n", " ")
                prompt = self.prompt_template.format(
                    context=sample_clean,
                    sentence=sentence
                )
                
                # 2. Tokenisation
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=1024  # Limite pour Ã©viter les erreurs
                ).to(self.device)
                
                # 3. GÃ©nÃ©ration
                try:
                    output_ids = self.model.generate(
                        inputs.input_ids,
                        max_new_tokens=max_new_tokens,
                        do_sample=False,  # DÃ©terministe
                        pad_token_id=self.tokenizer.pad_token_id
                    )
                    
                    # 4. DÃ©codage
                    output_text = self.tokenizer.decode(
                        output_ids[0],
                        skip_special_tokens=True
                    )
                    
                    # 5. Extraction de la rÃ©ponse
                    response = output_text.replace(prompt, "").strip()
                    
                    # 6. Post-traitement et scoring
                    score = self._text_to_score(response)
                    scores_matrix[sent_idx, sample_idx] = score
                    
                    if batch_responses:
                        sentence_responses.append(response)
                
                except Exception as e:
                    print(f"\nâš ï¸  Erreur pour phrase {sent_idx}, sample {sample_idx}: {e}")
                    scores_matrix[sent_idx, sample_idx] = 0.5  # Score neutre en cas d'erreur
                    if batch_responses:
                        sentence_responses.append("ERROR")
            
            if batch_responses:
                all_responses.append(sentence_responses)
        
        # Moyenne sur tous les Ã©chantillons pour chaque phrase
        final_scores = scores_matrix.mean(axis=1)
        
        if batch_responses:
            return final_scores, all_responses
        
        return final_scores
    
    def _text_to_score(self, text: str) -> float:
        """
        Convertit la rÃ©ponse textuelle en score
        
        Args:
            text: RÃ©ponse gÃ©nÃ©rÃ©e par le modÃ¨le
            
        Returns:
            score: 0.0 (yes), 1.0 (no), ou 0.5 (autre)
        """
        text_clean = text.lower().strip()
        
        # VÃ©rifie si commence par "yes"
        if text_clean[:3] == 'yes':
            return self.text_mapping['yes']
        
        # VÃ©rifie si commence par "no"
        elif text_clean[:2] == 'no':
            return self.text_mapping['no']
        
        # Tout le reste
        else:
            # Log les rÃ©ponses inattendues (une seule fois)
            if text_clean not in self.unknown_responses:
                print(f"\nâš ï¸  RÃ©ponse inattendue: '{text_clean}'")
                self.unknown_responses.add(text_clean)
            return self.text_mapping['n/a']
    
    def evaluate_passage(
        self,
        passage: str,
        sampled_passages: List[str],
        sentence_splitter=None,
        verbose: bool = True
    ) -> dict:
        """
        Ã‰value un passage complet (dÃ©coupage en phrases automatique)
        
        Args:
            passage: Texte Ã  Ã©valuer
            sampled_passages: Passages Ã©chantillonnÃ©s pour comparaison
            sentence_splitter: Fonction de dÃ©coupage (si None, utilise split simple)
            verbose: Afficher les dÃ©tails
            
        Returns:
            dict avec 'sentences', 'scores', 'mean_score'
        """
        # DÃ©coupage en phrases
        if sentence_splitter is None:
            # DÃ©coupage simple (Ã  amÃ©liorer avec spacy si besoin)
            sentences = [s.strip() + '.' for s in passage.split('.') if s.strip()]
        else:
            sentences = sentence_splitter(passage)
        
        if verbose:
            print(f"\nðŸ“„ Ã‰valuation de {len(sentences)} phrases...")
        
        # Calcul des scores
        scores = self.predict(sentences, sampled_passages, verbose=verbose)
        
        # RÃ©sultats
        results = {
            'sentences': sentences,
            'scores': scores.tolist(),
            'mean_score': float(scores.mean()),
            'max_score': float(scores.max()),
            'num_hallucinations': int((scores > 0.5).sum())  # Seuil arbitraire
        }
        
        if verbose:
            print(f"\nðŸ“Š RÃ©sultats:")
            print(f"   Score moyen: {results['mean_score']:.3f}")
            print(f"   Score max: {results['max_score']:.3f}")
            print(f"   Hallucinations potentielles: {results['num_hallucinations']}")
        
        return results


def test_simple():
    """Test rapide de la classe"""
    print("=" * 60)
    print("TEST SIMPLE DE SELFCHECKPROMPT")
    print("=" * 60)
    
    # Initialisation
    checker = SelfCheckPrompt(model_name="gpt2")
    
    # DonnÃ©es de test
    sentences = [
        "Michael Alan Weiner was born in 1942.",
        "Michael Alan Weiner was born in 1960.",  # Contradiction
    ]
    
    sampled_passages = [
        "Michael Alan Weiner was born in 1942 and is a radio host.",
        "Michael Alan Weiner was born in 1942 in New York.",
        "Michael Weiner, born 1942, hosts The Savage Nation.",
    ]
    
    # Ã‰valuation
    scores = checker.predict(sentences, sampled_passages, verbose=True)
    
    # Affichage
    print("\n" + "=" * 60)
    print("RÃ‰SULTATS:")
    print("=" * 60)
    for i, (sent, score) in enumerate(zip(sentences, scores)):
        status = "âœ… COHÃ‰RENT" if score < 0.5 else "âŒ INCOHÃ‰RENT"
        print(f"\nPhrase {i+1}: {sent}")
        print(f"Score: {score:.3f} {status}")


if __name__ == "__main__":
    test_simple()