# Statapp Run code

## üêç Python Version

Developed with:

```bash
Python 3.12.7
```

---

## üì¶ Installer les d√©pendances

Depuis la racine du projet :

```bash
pip install -r requirements.txt
```

---

## üîë Fichier `.env`

Pour faire marcher les diff√©rents codes, il faut cr√©er un fichier `.env` √† la racine du projet avec :

```env
GEMINI_API_KEY="votre_cl√©"
```

Vous pouvez voir un exemple dans le fichier `env_example.sh` :

```bash
# Create an AI Studio API KEY on https://aistudio.google.com/apikey
GEMINI_API_KEY="Your_API_KEY"
```


---

## 1. √âvaluer un score de confiance avec les m√©thodes white-box

L‚Äôid√©e de cette partie du projet est de **mesurer la confiance du mod√®le** pour chaque r√©ponse en exploitant les **log-probabilit√©s des tokens** retourn√©es par Gemini.

### Principe g√©n√©ral

1. On donne au mod√®le une liste de questions (*queries*).
2. On force la r√©ponse dans un format JSON strict :

   - Mode `bool` :

     ```json
     {"answers":[true,false,true]}
     ```

   - Mode `float` :

     ```json
     {"answers":[4.25,0.3333,0.015]}
     ```

   - Mode `string` (QCM) :

     ```json
     {"answers":["D","C","A"]}
     ```

3. `UnifiedProbGeminiBatch` parse ce JSON pour r√©cup√©rer la **valeur** de chaque r√©ponse.
4. En parall√®le, le code r√©cup√®re les **tokens choisis** et leurs **logprobs**.
5. En agr√©geant ces logprobs (en gros `exp(sum(logprobs))`), on obtient un **score de confiance** pour chaque r√©ponse.
6. Le script retourne pour chaque query un couple :

   ```text
   (valeur, probabilit√©)
   ```
---

## üìÅ Fichiers importants

- `white_box.py`  
  Contient la classe principale `UnifiedProbGeminiBatch` qui :
  - construit les prompts pour Gemini,
  - appelle l‚ÄôAPI avec `response_logprobs=True`,
  - parse le JSON de sortie,
  - extrait les tokens + logprobs,
  - calcule un score de confiance pour chaque r√©ponse.

- `main_whitebox.py`  
  Script en ligne de commande qui montre comment utiliser `UnifiedProbGeminiBatch` pour :
  - des r√©ponses bool√©ennes (`mode=bool`),
  - des r√©ponses num√©riques (`mode=float`),
  - des r√©ponses QCM (`mode=string`).

---

## ‚ñ∂Ô∏è Comment lancer `main_whitebox.py`

Assurez-vous d‚Äôabord que :

- l‚Äôenvironnement Python est activ√©,
- les d√©pendances sont install√©es,
- le fichier `.env` avec `GEMINI_API_KEY` est pr√©sent.

Ensuite, depuis la racine du projet :

### üîπ Mode bool√©en

Questions dont la r√©ponse attendue est `True` ou `False`.

```bash
python main_whitebox.py --mode bool
```

---

### üîπ Mode num√©rique (`float`)

Questions qui doivent renvoyer un **float**.

```bash
python -m statap_code.main_whitebox --mode float
```

---

### üîπ Mode QCM (`string`)

Questions √† choix multiples o√π la r√©ponse est une lettre (`A`, `B`, `C`, `D`, ‚Ä¶).

```bash
python -m statap_code.main_whitebox --mode string
```
---

## ‚öôÔ∏è Arguments disponibles

```bash
python -m statap_code.main_whitebox   --mode {bool,float,string}   --model models/gemini-2.0-flash   --batch-size 16   --log-level INFO
```

---

## üì§ Format de sortie

Exemple d‚Äôaffichage :

```text
01. Is 7 * 8 equal to 56?
    ‚Üí (True, 0.998231)

02. Does 2^10 equal 1000?
    ‚Üí (False, 0.957221)
```

---



## 2. Evaluer les phrases g√©n√©r√©es par un LLM (SelfCheckGPT)

Cette m√©thode suit l‚Äôesprit du papier **SelfCheckGPT (2023)** :  
un LLM joue le r√¥le de *juge* pour v√©rifier si une phrase est **support√©e** par un *ensemble de passages*.

Pour chaque phrase *s_i* et passage *p_j* :

- `true`  ‚Üí support√©  
- `false` ‚Üí non support√©  
- parse KO ‚Üí score neutre `0.5`

Le mod√®le doit r√©pondre **uniquement** :

```json
{"answers":[true,false,true,...]}
```

---


## üìÅ Fichiers importants

- `LLM_judge_gemini.py`  
  Contient la classe `SelfCheckGeminiBatch`.

- `main_LLM_judge.py`  
  Script permettant de lancer une √©valuation SelfCheck.

---

## Scores retourn√©s

- `1.0` si support√©  
- `0.0` si non support√©  
- `0.5` si ind√©termin√©  

Renvoie :

- `predict_matrix(sentences, passages)` ‚Üí matrice **M √ó K**
- `predict_mean(sentences, passages)` ‚Üí score moyen par phrase

---

## Exemple r√©aliste (inspir√© SelfCheckGPT)

Nous voulons v√©rifier si un mod√®le hallucine dans un r√©sum√© g√©n√©r√©.

### üîπ Phrases (r√©sum√© du mod√®le)

```text
1. The Eiffel Tower was completed in 1889.
2. The Eiffel Tower is 450 meters tall.
3. It was originally intended as a temporary structure.
4. It was designed by Antoni Gaud√≠.
```

### üîπ Passages (sources candidates)

```text
A. The Eiffel Tower, designed by Gustave Eiffel for the 1889 Exposition Universelle 
   in Paris, was completed the same year and quickly became a global symbol of France.

B. When it was built, the Eiffel Tower was initially intended to be dismantled after 
   twenty years. However, its usefulness as a radio transmission tower ensured its preservation.

C. The Eiffel Tower stands at approximately 324 meters tall, including its antenna. 
   It remained the tallest man-made structure in the world until 1930.

```

Ici :

- Phrase 1 ‚Üí support√©e (A)  
- Phrase 2 ‚Üí **non support√©e** car la tour ne fait pas 450m  
- Phrase 3 ‚Üí support√©e (B)  
- Phrase 4 ‚Üí **hallucination claire** (Gaud√≠ n‚Äôa rien √† voir)

---

## ‚ñ∂Ô∏è Lancer l‚Äôexemple SelfCheck

Ex√©cuter :

```bash
python -m statap_code.main_LLM_judge   
```

Ou avec param√®tres :

```bash
python -m statap_code.main_LLM_judge --batch-size 16 --log-level INFO
```

---

## üì§ Exemple de sortie attendue

```
=== Score matrix (M x K) ===
[[1. 0. 0.]
 [0. 0. 0.]
 [0. 1. 0.]
 [0. 0. 0.]]

=== Mean score per sentence ===
[0.33   0.   0.33 0.  ]

- The Eiffel Tower was completed in 1889.
  score=0.333

- The Eiffel Tower is 450 meters tall.
  score=0.000

- It was originally intended as a temporary structure.
  score=0.333

- It was designed by Antoni Gaud√≠.
  score=0.000
```


